{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(100)\n",
    "torch.manual_seed(100)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = 'data/all_couplets.txt'\n",
    "vocab_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(raw_data, test_size=3000):\n",
    "    with open(raw_data, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    lines = list(map(str.strip, lines))\n",
    "    \n",
    "    np.random.shuffle(lines)\n",
    "    \n",
    "    train_lines = lines[test_size:]\n",
    "    test_lines = lines[:test_size]\n",
    "        \n",
    "    return train_lines, test_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771491, 3000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines, test_lines = split_dataset(raw_data, test_size=3000)\n",
    "len(train_lines), len(test_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取字符表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(train_lines, size=-1):\n",
    "    counter = Counter(''.join(train_lines))\n",
    "    vocab = sorted(counter, key=lambda c: counter[c], reverse=True)\n",
    "    \n",
    "    if size != -1:\n",
    "        vocab = vocab[:size]\n",
    "    \n",
    "    print(f\"last character: {vocab[-1]}, frequency: {counter[vocab[-1]]}\")\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last character: 辣, frequency: 771\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(train_lines, size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_char(vocab):\n",
    "    vocab = vocab.copy()\n",
    "    vocab = ['<MASK>', '<UNK>', '<BOS>', '<EOS>'] + vocab\n",
    "    return dict(zip(range(0, len(vocab)), vocab)), dict(zip(vocab, range(0, len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2char, char2index = create_index_char(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Couplets_dataset(Dataset):\n",
    "    def __init__(self, lines, char2index, min_len=10, max_len=20):\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        ups = []\n",
    "        downs = []\n",
    "        \n",
    "        for line in lines:  \n",
    "            if '；' in line:\n",
    "                line_split = tuple(line.split('；'))\n",
    "                if len(line_split) == 2:\n",
    "                    up, down = line_split\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            down = down[:-1]\n",
    "            if len(up) != len(down):\n",
    "                continue\n",
    "                \n",
    "            up_len = len(up)\n",
    "            if up_len < min_len or up_len > max_len:\n",
    "                continue\n",
    "            \n",
    "            ups.append(up)\n",
    "            downs.append(down)\n",
    "            \n",
    "        self.ups = ups\n",
    "        self.downs = downs\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        up, down = self.ups[index], self.downs[index]\n",
    "        down = ['<BOS>'] + list(down) + ['<EOS>']\n",
    "        \n",
    "        up_len = len(up)\n",
    "        if up_len < self.max_len:\n",
    "            up = list(up) + ['<MASK>'] * (self.max_len - up_len)\n",
    "            down = down + ['<MASK>'] * (self.max_len - up_len)\n",
    "        \n",
    "        x = torch.tensor([char2index.get(c, char2index['<UNK>']) for c in up], dtype=torch.long)\n",
    "        y = torch.tensor([char2index.get(c, char2index['<UNK>']) for c in down])\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644701, 2481)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = Couplets_dataset(train_lines, char2index, min_len=5, max_len=15)\n",
    "test_set = Couplets_dataset(test_lines, char2index, min_len=5, max_len=15)\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15]), torch.Size([17]))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "X, Y = train_set[6]\n",
    "X.size(), Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>',\n",
       " '诚',\n",
       " '心',\n",
       " '待',\n",
       " '民',\n",
       " '铸',\n",
       " '<UNK>',\n",
       " '魂',\n",
       " '<EOS>',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "[index2char[i.item()] for i in train_set[6][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 15]), torch.Size([256, 17]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "X, Y = next(iter(train_loader))\n",
    "X.size(), Y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Couplet_encoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=200, layer_num=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = embedding_layer\n",
    "        self.bilstm = nn.LSTM(embedding_layer.weight.size(1), hidden_dim, layer_num,\n",
    "                              batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = self.bilstm(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 15, 400]),\n",
       " torch.Size([4, 256, 200]),\n",
       " torch.Size([4, 256, 200]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "embedding_layer = nn.Embedding(len(char2index), 100)\n",
    "encoder = Couplet_encoder(embedding_layer)\n",
    "\n",
    "X, Y = next(iter(train_loader))\n",
    "output, (h_n, c_n) = encoder(X)\n",
    "output.size(), h_n.size(), c_n.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_layer(nn.Module):\n",
    "    def __init__(self, encoder_output_dim, decoder_h_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(encoder_output_dim + decoder_h_dim, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "        \n",
    "    def forward(self, encoder_output, decoder_h):\n",
    "        decoder_h_repeat = torch.stack([decoder_h] * encoder_output.size(1), 1)\n",
    "        merge = torch.cat((encoder_output, decoder_h_repeat), dim=2)\n",
    "        merge = merge.view(merge.size(0) * merge.size(1), merge.size(2))\n",
    "        \n",
    "        X = self.fc(merge).view(encoder_output.size(0), encoder_output.size(1))\n",
    "        attention_weights = self.softmax(X).view(X.size(0), X.size(1), 1)\n",
    "        \n",
    "        return (attention_weights * encoder_output).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 40])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "encoder_output = torch.randn(100, 15, 40)\n",
    "decoder_h = torch.randn(100, 40)\n",
    "\n",
    "attention_layer = Attention_layer(encoder_output.size(-1), decoder_h.size(-1))\n",
    "attention_layer(encoder_output, decoder_h).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Couplet_decoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, attention_layer, encoder_output_dim, \n",
    "                 encoder_state_dim, bos_index, hidden_dim=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bos_index = bos_index\n",
    "        \n",
    "        self.embedding = embedding_layer\n",
    "        self.attention = attention_layer\n",
    "        \n",
    "        self.h_project_fc = nn.Linear(encoder_state_dim, hidden_dim * 2)\n",
    "        self.c_project_fc = nn.Linear(encoder_state_dim, hidden_dim * 2)\n",
    "        \n",
    "        self.lstm_cell_0 = nn.LSTMCell(embedding_layer.weight.size(1) + encoder_output_dim, hidden_dim)\n",
    "        self.lstm_cell_1 = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, embedding_layer.weight.size(0))\n",
    "        \n",
    "    def forward(self, X, encoder_output, encoder_last_state):\n",
    "        encoder_last_h, encoder_last_c = encoder_last_state\n",
    "        decoder_input_h, decoder_input_c = self.h_project_fc(encoder_last_h), self.c_project_fc(encoder_last_c)\n",
    "        \n",
    "        decoder_h_0, decoder_h_1 = decoder_input_h[:, :self.hidden_dim], decoder_input_h[:, self.hidden_dim:]\n",
    "        decoder_c_0, decoder_c_1 = decoder_input_c[:, :self.hidden_dim], decoder_input_h[:, self.hidden_dim:]\n",
    "        \n",
    "        X_step = torch.full((X.size(0),), self.bos_index, dtype=torch.long, device=device)\n",
    "\n",
    "        outputs = []        \n",
    "        for i in range(X.size(1)):\n",
    "            embedding_step = self.embedding(X_step)\n",
    "            attention_step = self.attention(encoder_output, torch.cat([decoder_h_0, decoder_h_1], dim=1))\n",
    "\n",
    "            input_step = torch.cat([embedding_step, attention_step], dim=1)\n",
    "            \n",
    "            decoder_h_0, decoder_c_0 = self.lstm_cell_0(input_step, (decoder_h_0, decoder_c_0))\n",
    "            decoder_h_1, decoder_c_1 = self.lstm_cell_1(decoder_h_0, (decoder_h_1, decoder_c_1))\n",
    "            Y_step = self.fc(decoder_h_1)\n",
    "            outputs.append(Y_step)\n",
    "            \n",
    "            X_step = X[:, i]\n",
    "            \n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 15, 2004])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "X = torch.ones(100, 15, dtype=torch.long)\n",
    "encoder_last_state = torch.randn(100, 20), torch.randn(100, 20)\n",
    "\n",
    "encoder_output = torch.randn(100, 15, 40)\n",
    "decoder_h = torch.randn(100, 80)\n",
    "attention_layer = Attention_layer(encoder_output.size(-1), decoder_h.size(-1))\n",
    "\n",
    "embedding_layer = nn.Embedding(len(char2index), 120)\n",
    "\n",
    "\n",
    "decoder = Couplet_decoder(embedding_layer, attention_layer, encoder_output_dim=40, encoder_state_dim=20,\n",
    "                          bos_index=char2index['<BOS>'], hidden_dim=40)\n",
    "decoder(X, encoder_output, encoder_last_state).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam_searcher(nn.Module):\n",
    "    def __init__(self, decoder, eos_index):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder = decoder\n",
    "        self.eos_index = eos_index\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "    def _beam_score(self, Y_prob, alpha=0.7):\n",
    "        return ((1 / Y_prob.size(1)) ** alpha) * Y_prob.log().sum(dim=1)\n",
    "    \n",
    "    def _pick_beam_width(self, best_probs_list, step_probs_list, alpha=0.7):\n",
    "        score_indices = []\n",
    "        \n",
    "        for i in range(len(best_probs_list)):\n",
    "            best_probs = best_probs_list[i]\n",
    "            step_probs = step_probs_list[i]\n",
    "            step_prob_list = list(torch.split(step_probs, 1, dim=1))\n",
    "            \n",
    "            for j in range(len(step_prob_list)):\n",
    "                probs = torch.cat([best_probs, step_prob_list[j]], dim=1)\n",
    "                score = self._beam_score(probs, alpha).item()               \n",
    "                score_indices.append((score, i, j))\n",
    "\n",
    "        sorted_score_indices = sorted(score_indices, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        return sorted_score_indices\n",
    "        \n",
    "    def forward(self, encoder_output, encoder_last_state, beam_width=3, alpha=0.7, max_len=15):\n",
    "        encoder_last_h, encoder_last_c = encoder_last_state\n",
    "        decoder_input_h = self.decoder.h_project_fc(encoder_last_h)\n",
    "        decoder_input_c = self.decoder.c_project_fc(encoder_last_c)\n",
    "        \n",
    "        hidden_dim = decoder_input_h.size(-1) // 2\n",
    "        \n",
    "        decoder_h_0, decoder_h_1 = decoder_input_h[:, :hidden_dim], decoder_input_h[:, hidden_dim:]\n",
    "        decoder_c_0, decoder_c_1 = decoder_input_c[:, :hidden_dim], decoder_input_h[:, hidden_dim:]\n",
    "        \n",
    "        best_output_list = [torch.tensor([[self.eos_index]] * encoder_output.size(0), \n",
    "                                         dtype=torch.long, device=device, requires_grad=False)]\n",
    "        best_probs_list = None\n",
    "        best_state_0_list = [(decoder_h_0, decoder_c_0)]\n",
    "        best_state_1_list = [(decoder_h_1, decoder_c_1)]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            step_probs_list = []\n",
    "            step_indices_list = []\n",
    "            step_state_0_list = []\n",
    "            step_state_1_list = []\n",
    "            \n",
    "            for i in range(1 if best_probs_list is None else beam_width):\n",
    "                X_step = best_output_list[i][:, -1]\n",
    "                decoder_h_0, decoder_c_0 = best_state_0_list[i]\n",
    "                decoder_h_1, decoder_c_1 = best_state_1_list[i]\n",
    "            \n",
    "                embedding_step = self.decoder.embedding(X_step)\n",
    "                attention_step = self.decoder.attention(encoder_output, \n",
    "                                                        torch.cat([decoder_h_0, decoder_h_1], dim=1))\n",
    "                input_step = torch.cat([embedding_step, attention_step], dim=1)\n",
    "            \n",
    "                decoder_h_0, decoder_c_0 = self.decoder.lstm_cell_0(input_step, (decoder_h_0, decoder_c_0))\n",
    "                decoder_h_1, decoder_c_1 = self.decoder.lstm_cell_1(decoder_h_0, (decoder_h_1, decoder_c_1))\n",
    "                Y_step = self.decoder.fc(decoder_h_1)\n",
    "                Y_step = self.softmax(Y_step)\n",
    "                \n",
    "                sorted_probs, sorted_indices = Y_step.sort(dim=1, descending=True)\n",
    "                step_probs = sorted_probs[:, :beam_width]\n",
    "                step_indices = sorted_indices[:, :beam_width]\n",
    "                \n",
    "                step_probs_list.append(step_probs)\n",
    "                step_indices_list.append(step_indices)                \n",
    "                step_state_0_list.append((decoder_h_0, decoder_c_0))\n",
    "                step_state_1_list.append((decoder_h_1, decoder_c_1))\n",
    "                \n",
    "            if best_probs_list is None:\n",
    "                best_probs_list = list(torch.split(step_probs_list[0], 1, dim=1))\n",
    "                best_output_list = list(torch.split(step_indices_list[0], 1, dim=1))\n",
    "                best_state_0_list = step_state_0_list * 3\n",
    "                best_state_1_list = step_state_1_list * 3\n",
    "            else:\n",
    "                sorted_score_indices = self._pick_beam_width(best_probs_list, step_probs_list, alpha)\n",
    "                    \n",
    "                temp_best_probs_list = []\n",
    "                temp_best_output_list = []\n",
    "                best_state_0_list = []\n",
    "                best_state_1_list = []\n",
    "                for j in range(beam_width):\n",
    "                    score, ii, jj = sorted_score_indices[j]\n",
    "                    best_probs = torch.cat([best_probs_list[ii], \n",
    "                                            torch.split(step_probs_list[ii], 1, dim=1)[jj]],\n",
    "                                           dim=1)\n",
    "                    best_output = torch.cat([best_output_list[ii],\n",
    "                                           torch.split(step_indices_list[ii], 1, dim=1)[jj]],\n",
    "                                           dim=1),\n",
    "                    temp_best_probs_list.append(best_probs)\n",
    "                    temp_best_output_list.append(best_output[0])\n",
    "                    best_state_0_list.append(step_state_0_list[ii])\n",
    "                    best_state_1_list.append(step_state_1_list[ii])\n",
    "                    \n",
    "                best_probs_list = temp_best_probs_list.copy()\n",
    "                best_output_list = temp_best_output_list.copy()\n",
    "                \n",
    "        best_probs = torch.cat(best_probs_list, dim=0)\n",
    "        best_outputs = torch.cat(best_output_list, dim=0) \n",
    "        scores = self._beam_score(best_probs, alpha)\n",
    "        \n",
    "        return best_output[torch.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1775, 1775, 1775, 1077, 1076, 1076, 1076, 1076, 1076, 1076, 1064, 1076,\n",
       "         1064, 1064,  786]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "encoder_output = torch.randn(1, 15, 40)\n",
    "encoder_last_state = torch.randn(1, 20), torch.randn(1, 20)\n",
    "beam_searcher = Beam_searcher(decoder, char2index['<EOS>'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = beam_searcher(encoder_output, encoder_last_state)\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Couplet_net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, eos_index):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = Attention_layer(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.encoder = Couplet_encoder(self.embedding, hidden_dim, 2)\n",
    "        self.decoder = Couplet_decoder(self.embedding, self.attention, hidden_dim * 2, hidden_dim * 4, hidden_dim)\n",
    "        self.beamsearcher = Beam_searcher(self.decoder, eos_index)\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        encoder_output, (encoder_h, encoder_c) = self.encoder(X)\n",
    "        encoder_h = encoder_h.transpose(1, 0)\n",
    "        encoder_h = encoder_h.reshape(encoder_h.size(0), -1)\n",
    "        encoder_c = encoder_c.transpose(1, 0)\n",
    "        encoder_c = encoder_c.reshape(encoder_c.size(0), -1)\n",
    "        encoder_last_state = (encoder_h, encoder_c)\n",
    "        \n",
    "        decoder_output = self.decoder(Y, encoder_output, encoder_last_state)\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "    def beam_search(self, X, beam_width=3, alpha=0.7, max_len=15):\n",
    "        encoder_output, (encoder_h, encoder_c) = self.encoder(X)\n",
    "        encoder_h = encoder_h.transpose(1, 0)\n",
    "        encoder_h = encoder_h.reshape(encoder_h.size(0), -1)\n",
    "        encoder_c = encoder_c.transpose(1, 0)\n",
    "        encoder_c = encoder_c.reshape(encoder_c.size(0), -1)\n",
    "        encoder_last_state = (encoder_h, encoder_c)\n",
    "        \n",
    "        output = self.beamsearcher(encoder_output, encoder_last_state, beam_width, alpha, max_len)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10, 2004])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "net = Couplet_net(len(char2index), 100, 200, char2index['<BOS>'])\n",
    "\n",
    "X = torch.ones(50, 10, dtype=torch.long)\n",
    "Y = torch.ones(50, 10, dtype=torch.long)\n",
    "net(X, Y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "X = torch.ones(1, 15, dtype=torch.long)\n",
    "net.beam_search(X).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 17, 2004])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "X, Y = next(iter(train_loader))\n",
    "net(X, Y).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Couplet_net(len(char2index), 100, 200, char2index['<BOS>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 搜索最大学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lr_finder:\n",
    "    def __init__(self, model, dataloader, loss_fn, optimizer):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = model.to('cuda')\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def lr_find(self, steps=150, repeat=5, lr_range=(1e-4, 10), plot=True):\n",
    "        self.model.train()\n",
    "\n",
    "        old_state = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        history_loss = []\n",
    "        history_lr = []\n",
    "\n",
    "        for i in range(repeat):\n",
    "            step = 0\n",
    "            losses = []\n",
    "            lrs = []\n",
    "            stop_flag = False\n",
    "            while True:\n",
    "                for X, Y in self.dataloader:\n",
    "                    if torch.cuda.is_available():\n",
    "                        X, Y = X.to('cuda'), Y.to('cuda')\n",
    "\n",
    "                    lr = self.exp_lr_scheduler(step, steps, lr_range)\n",
    "                    self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "                    outputs = self.model(X, Y)\n",
    "                    loss = self.loss_fn(outputs, Y)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    losses.append(loss.item())\n",
    "                    lrs.append(lr)\n",
    "\n",
    "                    step += 1\n",
    "                    if step == steps:\n",
    "                        stop_flag = True\n",
    "                        break\n",
    "\n",
    "                if stop_flag:\n",
    "                    break\n",
    "\n",
    "            self.model.load_state_dict(old_state)\n",
    "\n",
    "            history_loss.append(losses)\n",
    "            history_lr.append(lrs)\n",
    "\n",
    "        x = np.array(history_lr).mean(axis=0)\n",
    "        y = np.array(history_loss).mean(axis=0)\n",
    "\n",
    "        if plot:\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('lr')\n",
    "            plt.ylabel('loss')\n",
    "            plt.title('loss_lr curve')\n",
    "            plt.plot(x, y)\n",
    "        else:\n",
    "            return x, y\n",
    "\n",
    "    def exp_lr_scheduler(self, step, steps, lr_range):\n",
    "        pct = step / (steps - 1)\n",
    "        return lr_range[0] * (lr_range[1] / lr_range[0]) ** pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss():\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def new_loss_fn(outputs, labels):\n",
    "        outputs = outputs.reshape((-1, outputs.size(-1)))\n",
    "        labels = labels.reshape((-1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        return loss\n",
    "    \n",
    "    return new_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = Lr_finder(net, train_loader, create_loss(),\n",
    "                      optim.SGD(net.parameters(), 0.1, weight_decay=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.lr_find(steps=10, lr_range=(1e-5, 1), plot=True, repeat=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model):\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = model.to('cuda')\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def fit(self, dataloader, lr, epochs, weight_decay=0, print_steps=200):\n",
    "        self.model.train()\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr, momentum=0.9,\n",
    "                              weight_decay=weight_decay, nesterov=False)\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, lr, epochs=epochs,\n",
    "                                            steps_per_epoch=len(dataloader))\n",
    "\n",
    "        history_loss = []\n",
    "        history_steps = []\n",
    "        for epoch in range(epochs):\n",
    "            for step, (X, Y) in enumerate(dataloader):\n",
    "                if torch.cuda.is_available():\n",
    "                    X, Y = X.to('cuda'), Y.to('cuda')\n",
    "\n",
    "                outputs = self.model(X, Y)\n",
    "                outputs = outputs.reshape((-1, outputs.size(-1)))\n",
    "                Y = Y.reshape((-1))\n",
    "                loss = loss_fn(outputs, Y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                if step % print_steps == print_steps - 1:\n",
    "                    history_loss.append(loss.item())\n",
    "                    history_steps.append(epoch * len(dataloader) + step + 1)\n",
    "                    print(f\"epoch: {epoch + 1}    \\tstep: {step + 1}    \\tloss: {loss:.4f}\")\n",
    "\n",
    "        return history_steps, history_loss\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, Y in dataloader:\n",
    "                if torch.cuda.is_available():\n",
    "                    X, Y = X.to('cuda'), Y.to('cuda')\n",
    "\n",
    "                outputs = self.model(X)\n",
    "                outputs = outputs.reshape((-1, outputs.size(-1)))\n",
    "                Y = Y.reshape((-1))\n",
    "                loss = loss_fn(outputs, Y)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return np.average(losses)\n",
    "\n",
    "    \n",
    "    def beam_search(self, up, beam_width=3, alpha=0.7, max_len=15):\n",
    "        up = [char2index[x] for x in up]\n",
    "        up = torch.tensor(up, dtype=torch.long, device=device)\n",
    "        up = up.view(1, -1)\n",
    "        \n",
    "        down = self.model.beam_search(up, beam_width, alpha, max_len).squeeze().cpu().numpy()\n",
    "        down = [index2char[x] for x in down if x != char2index['<MASK>']]\n",
    "        \n",
    "        return ''.join(down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1    \tstep: 10    \tloss: 7.5820\n",
      "epoch: 1    \tstep: 20    \tloss: 7.5687\n",
      "epoch: 1    \tstep: 30    \tloss: 7.5488\n",
      "epoch: 1    \tstep: 40    \tloss: 7.5287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-309-c68d1c2b3cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-307-2e3ef102689f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataloader, lr, epochs, weight_decay, print_steps)\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-256-b4d2dc14bd58>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mencoder_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mencoder_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c074d7efe90b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(train_loader, 0.01, 1, weight_decay=0, print_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0 1495\n",
      "    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'杖'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.beam_search('白日依山尽')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
